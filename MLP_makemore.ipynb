{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in all the words\n",
    "words = open('names.txt','r').read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "print(itos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma\n",
      "... ----> e\n",
      "..e ----> m\n",
      ".em ----> m\n",
      "emm ----> a\n",
      "mma ----> .\n",
      "olivia\n",
      "... ----> o\n",
      "..o ----> l\n",
      ".ol ----> i\n",
      "oli ----> v\n",
      "liv ----> i\n",
      "ivi ----> a\n",
      "via ----> .\n",
      "ava\n",
      "... ----> a\n",
      "..a ----> v\n",
      ".av ----> a\n",
      "ava ----> .\n",
      "isabella\n",
      "... ----> i\n",
      "..i ----> s\n",
      ".is ----> a\n",
      "isa ----> b\n",
      "sab ----> e\n",
      "abe ----> l\n",
      "bel ----> l\n",
      "ell ----> a\n",
      "lla ----> .\n",
      "sophia\n",
      "... ----> s\n",
      "..s ----> o\n",
      ".so ----> p\n",
      "sop ----> h\n",
      "oph ----> i\n",
      "phi ----> a\n",
      "hia ----> .\n",
      "tensor([0, 0, 5])\n",
      "tensor(13)\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "# block size is the context length: how many chars do we take to predict the next one\n",
    "block_size = 3 \n",
    "x, y =[],[]\n",
    "for w in words[:5]:\n",
    "    print(w)\n",
    "    context = [0] * block_size # initializing the block with ... padding\n",
    "    for ch in w + '.': #to mark the ending of a word\n",
    "        ix = stoi[ch]\n",
    "        x.append(context) # this is where it is intially padded with dots....\n",
    "        y.append(ix) # index of the next character\n",
    "        print(''.join(itos[i] for i in context),'---->',itos[ix])\n",
    "        context = context[1:] + [ix] #crop and append the character for next iteration(decrease the padded dots)\n",
    "    \n",
    "x = torch.tensor(x)\n",
    "y = torch.tensor(y)\n",
    "print(x[1,:])\n",
    "print(y[1])\n",
    "#from the above print example you can see that X is a possible context\n",
    "#before the next character which is y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3]), torch.int64, torch.Size([32]), torch.int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, x.dtype, y.shape, y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.rand((27,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1758, 0.5261])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.7556, 0.9409],\n",
       "         [0.7556, 0.9409],\n",
       "         [0.7556, 0.9409]],\n",
       "\n",
       "        [[0.7556, 0.9409],\n",
       "         [0.7556, 0.9409],\n",
       "         [0.1758, 0.5261]],\n",
       "\n",
       "        [[0.7556, 0.9409],\n",
       "         [0.1758, 0.5261],\n",
       "         [0.6099, 0.8225]],\n",
       "\n",
       "        [[0.1758, 0.5261],\n",
       "         [0.6099, 0.8225],\n",
       "         [0.6099, 0.8225]],\n",
       "\n",
       "        [[0.6099, 0.8225],\n",
       "         [0.6099, 0.8225],\n",
       "         [0.0095, 0.9829]],\n",
       "\n",
       "        [[0.7556, 0.9409],\n",
       "         [0.7556, 0.9409],\n",
       "         [0.7556, 0.9409]],\n",
       "\n",
       "        [[0.7556, 0.9409],\n",
       "         [0.7556, 0.9409],\n",
       "         [0.4164, 0.7279]],\n",
       "\n",
       "        [[0.7556, 0.9409],\n",
       "         [0.4164, 0.7279],\n",
       "         [0.7187, 0.0331]],\n",
       "\n",
       "        [[0.4164, 0.7279],\n",
       "         [0.7187, 0.0331],\n",
       "         [0.3101, 0.6418]],\n",
       "\n",
       "        [[0.7187, 0.0331],\n",
       "         [0.3101, 0.6418],\n",
       "         [0.1881, 0.2812]],\n",
       "\n",
       "        [[0.3101, 0.6418],\n",
       "         [0.1881, 0.2812],\n",
       "         [0.3101, 0.6418]],\n",
       "\n",
       "        [[0.1881, 0.2812],\n",
       "         [0.3101, 0.6418],\n",
       "         [0.0095, 0.9829]],\n",
       "\n",
       "        [[0.7556, 0.9409],\n",
       "         [0.7556, 0.9409],\n",
       "         [0.7556, 0.9409]],\n",
       "\n",
       "        [[0.7556, 0.9409],\n",
       "         [0.7556, 0.9409],\n",
       "         [0.0095, 0.9829]],\n",
       "\n",
       "        [[0.7556, 0.9409],\n",
       "         [0.0095, 0.9829],\n",
       "         [0.1881, 0.2812]],\n",
       "\n",
       "        [[0.0095, 0.9829],\n",
       "         [0.1881, 0.2812],\n",
       "         [0.0095, 0.9829]],\n",
       "\n",
       "        [[0.7556, 0.9409],\n",
       "         [0.7556, 0.9409],\n",
       "         [0.7556, 0.9409]],\n",
       "\n",
       "        [[0.7556, 0.9409],\n",
       "         [0.7556, 0.9409],\n",
       "         [0.3101, 0.6418]],\n",
       "\n",
       "        [[0.7556, 0.9409],\n",
       "         [0.3101, 0.6418],\n",
       "         [0.2289, 0.9012]],\n",
       "\n",
       "        [[0.3101, 0.6418],\n",
       "         [0.2289, 0.9012],\n",
       "         [0.0095, 0.9829]],\n",
       "\n",
       "        [[0.2289, 0.9012],\n",
       "         [0.0095, 0.9829],\n",
       "         [0.6917, 0.6145]],\n",
       "\n",
       "        [[0.0095, 0.9829],\n",
       "         [0.6917, 0.6145],\n",
       "         [0.1758, 0.5261]],\n",
       "\n",
       "        [[0.6917, 0.6145],\n",
       "         [0.1758, 0.5261],\n",
       "         [0.7187, 0.0331]],\n",
       "\n",
       "        [[0.1758, 0.5261],\n",
       "         [0.7187, 0.0331],\n",
       "         [0.7187, 0.0331]],\n",
       "\n",
       "        [[0.7187, 0.0331],\n",
       "         [0.7187, 0.0331],\n",
       "         [0.0095, 0.9829]],\n",
       "\n",
       "        [[0.7556, 0.9409],\n",
       "         [0.7556, 0.9409],\n",
       "         [0.7556, 0.9409]],\n",
       "\n",
       "        [[0.7556, 0.9409],\n",
       "         [0.7556, 0.9409],\n",
       "         [0.2289, 0.9012]],\n",
       "\n",
       "        [[0.7556, 0.9409],\n",
       "         [0.2289, 0.9012],\n",
       "         [0.4164, 0.7279]],\n",
       "\n",
       "        [[0.2289, 0.9012],\n",
       "         [0.4164, 0.7279],\n",
       "         [0.1294, 0.2117]],\n",
       "\n",
       "        [[0.4164, 0.7279],\n",
       "         [0.1294, 0.2117],\n",
       "         [0.9734, 0.3416]],\n",
       "\n",
       "        [[0.1294, 0.2117],\n",
       "         [0.9734, 0.3416],\n",
       "         [0.3101, 0.6418]],\n",
       "\n",
       "        [[0.9734, 0.3416],\n",
       "         [0.3101, 0.6418],\n",
       "         [0.0095, 0.9829]]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[[5,6,7]] # torch has a functionality which lets u index with a list\n",
    "c[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[x].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding(first layer)\n",
    "emb = c[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden layer\n",
    "w1 = torch.randn((6,100))\n",
    "b1 = torch.randn(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view() is used to reshape (32,3,2) in to (32,6) for matrix multiplication purpose\n",
    "h =torch.tanh(emb.view(emb.shape[0],6) @ w1 + b1)\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final layer\n",
    "w2 = torch.randn((100,27))\n",
    "b2 = torch.randn(27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = h@w2 + b2\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# softmax of final layer\n",
    "counts = logits.exp()\n",
    "prob = counts / counts.sum(1, keepdims = True) \n",
    "prob[0].sum() # therefore nomrlaized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
